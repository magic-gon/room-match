{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions definitions\n",
    "In this section is where the similarity metrics are calculted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from Levenshtein import distance as lev_distance\n",
    "import openai\n",
    "\n",
    "# Configure OpenAI API key\n",
    "openai.api_key = \"my_api_key\"\n",
    "\n",
    "def calculate_embedding_similarity(description1: str, description2: str) -> float:\n",
    "    \"\"\"\n",
    "    It generates embeddings for the descriptions using OpenAI and calculates the cosine similarity between them.\n",
    "\n",
    "    Args:\n",
    "        description1 (str): First room description.\n",
    "        description2 (str): Second room description.\n",
    "\n",
    "    Returns:\n",
    "        float: Similarity score between the two descriptions (value between -1 and 1).\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_embedding(text: str) -> list:\n",
    "        \"\"\"Generates an embedding for the given text using OpenAI.\"\"\"\n",
    "        response = openai.Embedding.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        return response['data'][0]['embedding']\n",
    "\n",
    "    def cosine_similarity(embedding1: list, embedding2: list) -> float:\n",
    "        \"\"\"Calculates the cosine similarity between two embeddings.\"\"\"\n",
    "        embedding1 = np.array(embedding1)\n",
    "        embedding2 = np.array(embedding2)\n",
    "        similarity = np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
    "        return similarity\n",
    "    \n",
    "    # Generate embeddings for both descriptions\n",
    "    embedding1 = get_embedding(description1)\n",
    "    embedding2 = get_embedding(description2)\n",
    "    \n",
    "    # Calculate similarity between the embeddings\n",
    "    similarity_score = cosine_similarity(embedding1, embedding2)\n",
    "    \n",
    "    return similarity_score\n",
    "\n",
    "def compute_numeric_similarity(room_1, room_2):\n",
    "    # Calculate numerical similarity using Euclidean distance\n",
    "    scaler = StandardScaler()\n",
    "    scaled_rooms = scaler.fit_transform([room_1, room_2])\n",
    "    return np.linalg.norm(scaled_rooms[0] - scaled_rooms[1])\n",
    "\n",
    "def compute_categorical_similarity(room_1, room_2, categorical_columns):\n",
    "    # Calculate similarity for categorical features using Label Encoding\n",
    "    le = LabelEncoder()\n",
    "    similarities = []\n",
    "    for col in categorical_columns:\n",
    "        le.fit([room_1[col], room_2[col]])\n",
    "        encoded = le.transform([room_1[col], room_2[col]])\n",
    "        similarities.append(1 - pairwise_distances([encoded], metric=\"hamming\")[0][0])  # Similaridad de Hamming\n",
    "    return np.mean(similarities)\n",
    "\n",
    "def compute_levenshtein_similarity(description_1, description_2):\n",
    "    # Calculate similarity using Levenshtein distance\n",
    "    return 1 - lev_distance(description_1, description_2) / max(len(description_1), len(description_2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Prepare data for training the model\n",
    "X = []  # Here the calculated similarity metrics should be added\n",
    "y = []  # Here the labels should be added (0 or 1, depending on whether there is a match or not)\n",
    "\n",
    "# Example data for two rooms\n",
    "room_1 = {'price': 100, 'size': 25, 'city': 'Paris', 'description': 'A spacious room in the center of Paris'}\n",
    "room_2 = {'price': 110, 'size': 28, 'city': 'Paris', 'description': 'A large room located in downtown Paris'}\n",
    "\n",
    "# Similarity metrics calculation\n",
    "numeric_similarity = compute_numeric_similarity(room_1, room_2)\n",
    "categorical_similarity = compute_categorical_similarity(room_1, room_2, categorical_columns=['city'])\n",
    "text_similarity = calculate_embedding_similarity(room_1['description'], room_2['description'])\n",
    "lev_similarity = compute_levenshtein_similarity(room_1['description'], room_2['description'])\n",
    "\n",
    "X.append([numeric_similarity, categorical_similarity, text_similarity, lev_similarity])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Divide into training and test data. With stratify=y, we ensure that the proportion of classes is the same in both sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Define the XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss')\n",
    "\n",
    "# Define the hyperparameter search with RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'max_depth': randint(3, 10),           # Depht between 3 and 10\n",
    "    'learning_rate': uniform(0.01, 0.3),   # Learning rate between 0.01 and 0.3\n",
    "    'n_estimators': randint(50, 200),      # Number of trees between 50 and 200\n",
    "    'subsample': uniform(0.6, 0.4),        # Subsampling between 0.6 and 1.0\n",
    "    'colsample_bytree': uniform(0.6, 0.4)  # Subsampling of columns between 0.6 and 1.0\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,               # Number of combinations to try\n",
    "    scoring=f1_score,        # Use f1 score as evaluation metric\n",
    "    cv=3,                    # 3-fold cross-validation\n",
    "    n_jobs=-1,               # Use all available cores\n",
    "    random_state=42          # Set the seed for reproducibility\n",
    ")\n",
    "\n",
    "\n",
    "# Adjust the model to the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model after searching for hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Prediction and model evaluation\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Calculate AUC-ROC to evaluate performance in terms of binary classification\n",
    "y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"ROC-AUC: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Train the final model with the best set of hyperparameters found\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model to be used in the room matching system\n",
    "with open('room_match_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "print(\"Model trained and saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex-nuitee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
